{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 7\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFJKkJoNCktRkUEiSmgwKSVKTQSFJavpfkPjDgnPIJ7oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x224421c0860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.00010132]\n",
      "[0 1] [ 0.99576025]\n",
      "[1 0] [ 0.9971501]\n",
      "[1 1] [-0.00460315]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                \n",
    "        graph = plt.plot(error)\n",
    "        plt.show()\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
